{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "logistic-regression-ngrams.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "aQLNbVJcQMiY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Code for TensorBoard\n",
        "==="
      ]
    },
    {
      "metadata": {
        "id": "VkaMehYdIwIQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "bc318cc3-ff50-41ab-d29b-44eded55405f"
      },
      "cell_type": "code",
      "source": [
        "!rm -rf logs/\n",
        "!rm ngrok*\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-08-17 08:26:40--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\r\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.3.63.2, 52.4.95.48, 52.23.126.223, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.3.63.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5363700 (5.1M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]   5.11M  6.87MB/s    in 0.7s    \n",
            "\n",
            "2018-08-17 08:26:41 (6.87 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [5363700/5363700]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "47plbab5I2Jr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "LOG_DIR = 'logs'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zcq21uFLI9ju",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "inqmnVzbJD1h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2b4367e5-4702-4241-c2e7-521311818cb9"
      },
      "cell_type": "code",
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://cf7823e2.ngrok.io\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3Bh_PVLdQS4P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Code for Downloading data from Google Drive\n",
        "==="
      ]
    },
    {
      "metadata": {
        "id": "IZK7c1IwPXme",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "e7d4d26c-7689-4431-9231-220e007a380e"
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# choose a local (colab) directory to store the data.\n",
        "local_download_path = os.path.expanduser('~/data')\n",
        "try:\n",
        "  os.makedirs(local_download_path)\n",
        "except: pass\n",
        "\n",
        "# 2. Auto-iterate using the query syntax\n",
        "#    https://developers.google.com/drive/v2/web/search-parameters\n",
        "file_list = drive.ListFile(\n",
        "    {'q': \"'1tvzMVuTcdX4Kkf3mrCi_2oDBw9p6CR-C' in parents\"}).GetList()\n",
        "\n",
        "for f in file_list:\n",
        "  # 3. Create & download by id.\n",
        "  print('title: %s, id: %s' % (f['title'], f['id']))\n",
        "  fname = os.path.join(local_download_path, f['title'])\n",
        "  print('downloading to {}'.format(fname))\n",
        "  f_ = drive.CreateFile({'id': f['id']})\n",
        "  f_.GetContentFile(fname)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title: reviews.txt, id: 1CvN6sjPurbm4zKlWv_S8_qFFUmrwrSfI\n",
            "downloading to /content/data/reviews.txt\n",
            "title: labels.txt, id: 1MRk0UtvxsuChmi6txxg6OIzcWEMziXO6\n",
            "downloading to /content/data/labels.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1OfpqBtTQZA0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Preparing the Dataset\n",
        "==="
      ]
    },
    {
      "metadata": {
        "id": "G0f7tNdgIyyN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ss6xttoF7hkB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('./data/reviews.txt', 'r') as file:\n",
        "  features = file.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GoDv7SrAPjCa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "features = features.split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ej2p7FQeQepm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Getting trigrams"
      ]
    },
    {
      "metadata": {
        "id": "qBVEXKrW7u9d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n8GuWk817zDQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def vectorizer(text):\n",
        "  vec = CountVectorizer(binary=True, min_df=3, ngram_range=(3, 3), token_pattern='(?u)\\\\b[a-z0-9\\-\\_]{3,}\\\\b', stop_words='english')\n",
        "  vec.fit(text)\n",
        "  return vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-OYHI7L5VgKk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vec = vectorizer(features)\n",
        "ngrams = vec.transform(features).toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k9j2BFFG79Wf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "b60c7002-4d83-4372-db16-9be51358cda5"
      },
      "cell_type": "code",
      "source": [
        "ngrams"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "RFMNl8_XP08x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c13102b2-a901-46fe-e136-5cc24ac4a031"
      },
      "cell_type": "code",
      "source": [
        "ngrams.shape"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 15146)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "metadata": {
        "id": "mB-UuJMFQivM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## One-hot encoding the labels"
      ]
    },
    {
      "metadata": {
        "id": "unAcktV5P11x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('./data/labels.txt', 'r') as file:\n",
        "  labels = file.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NaGtq7biP9uN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "labels = np.array([1 if label == 'positive' else 0 for label in labels.split('\\n')])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6aQcyj2OQEbA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tIAzufoWQGko",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "labels = to_categorical(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dkF2GcB1QnPI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Splitting the dataset"
      ]
    },
    {
      "metadata": {
        "id": "OyZefu_VQJ4A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(ngrams, labels[0:25000], test_size=0.30, stratify=labels[0:25000])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T5LHtvCNRC8H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Building the Model\n",
        "==="
      ]
    },
    {
      "metadata": {
        "id": "HBZjZW-Z8E4u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D0CqLQEA8HWs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "with tf.name_scope('input'):\n",
        "  input_features = tf.placeholder(dtype=tf.float32, shape=[None, ngrams.shape[1]],\n",
        "                                 name='input_features')\n",
        "  input_labels = tf.placeholder(dtype=tf.float32, shape=[None, labels.shape[1]],\n",
        "                               name='input_labels')\n",
        "  \n",
        "  #neural-network\n",
        "  with tf.name_scope('model'):\n",
        "    weights = {\n",
        "        'hidden_layer': tf.Variable(tf.random_normal(shape=[ngrams.shape[1], 16]), name='weights'),\n",
        "        'output_layer': tf.Variable(tf.random_normal(shape=[16, labels.shape[1]]), name='weights')\n",
        "    }\n",
        "    biases = {\n",
        "        'hidden_layer': tf.Variable(tf.random_normal(shape=[16])),\n",
        "        'output_layer': tf.Variable(tf.random_normal(shape=[labels.shape[1]]))\n",
        "    }\n",
        "    hidden_layer = tf.add(tf.matmul(input_features, weights['hidden_layer']), biases['hidden_layer'])\n",
        "    hidden_layer = tf.nn.relu(hidden_layer)\n",
        "    output_layer = tf.add(tf.matmul(hidden_layer, weights['output_layer']), biases['output_layer'])\n",
        "    predictions = tf.nn.softmax(output_layer)\n",
        "\n",
        "    #logistic-regression\n",
        "#   with tf.name_scope('model'):\n",
        "#     weights = tf.Variable(tf.random_normal(shape=[ngrams.shape[1], labels.shape[1]]),\n",
        "#                           name='weights')\n",
        "#     biases = tf.Variable(tf.random_normal(shape=[labels.shape[1]]))\n",
        "#     linear_model = tf.add(tf.matmul(input_features, weights), biases)\n",
        "#     predictions = tf.nn.softmax(linear_model)\n",
        "    \n",
        "    \n",
        "with tf.name_scope('training_ops'):\n",
        "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "      logits=output_layer, labels=input_labels))\n",
        "  train_op = tf.train.AdamOptimizer(learning_rate=1e-2).minimize(loss)\n",
        "  tf.summary.scalar(name='loss', tensor=loss)\n",
        "\n",
        "with tf.name_scope('metrics'):\n",
        "  correct_prediction = tf.cast(tf.equal(tf.argmax(predictions, 1),\n",
        "                                         tf.argmax(input_labels, 1)), tf.float32)\n",
        "  accuracy_op = tf.reduce_mean(correct_prediction)\n",
        "  tf.summary.scalar(name='accuracy', tensor=accuracy_op)\n",
        "\n",
        "summary = tf.summary.merge_all()\n",
        "writer = tf.summary.FileWriter(logdir='logs', graph=tf.get_default_graph())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gS1pF5ir-nL_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "init_op = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m6WWWGO9-3Sy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def next_batch(batch_size, features, labels):\n",
        "  indices = np.arange(start=0, stop=features.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "  indices = indices[:batch_size]\n",
        "  return features[indices], labels[indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y8VSK1NN-rPx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "94727a26-f56b-4daa-fa88-3766776112a9"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(init_op)\n",
        "  \n",
        "  # epochs = 10\n",
        "  for epoch in range(10):\n",
        "    for mini_batch in range(int(x_train.shape[0] / 16)):\n",
        "      batch_x, batch_y = next_batch(batch_size=16, features=x_train, labels=y_train)\n",
        "      \n",
        "      _, train_loss, train_accuracy, train_summary = sess.run([train_op, loss,\n",
        "                                                               accuracy_op,\n",
        "                                                              summary],\n",
        "                                              feed_dict={input_features: batch_x,\n",
        "                                                        input_labels: batch_y})\n",
        "      writer.add_summary(summary=train_summary, global_step=mini_batch)\n",
        "    print('Epoch {}, loss : {}, accuracy : {}'.format(epoch, train_loss,\n",
        "                                                      train_accuracy))  \n",
        "  \n",
        "  test_accuracy = sess.run(accuracy_op, feed_dict={input_features: x_test, input_labels: y_test})\n",
        "  print('Test accuracy : {}'.format(test_accuracy))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, loss : 0.4289850890636444, accuracy : 0.8125\n",
            "Epoch 1, loss : 0.39783626794815063, accuracy : 0.6875\n",
            "Epoch 2, loss : 0.3418755531311035, accuracy : 0.75\n",
            "Epoch 3, loss : 0.1313452422618866, accuracy : 1.0\n",
            "Epoch 4, loss : 0.13870924711227417, accuracy : 0.875\n",
            "Epoch 5, loss : 0.1279546469449997, accuracy : 0.9375\n",
            "Epoch 6, loss : 0.23715220391750336, accuracy : 0.8125\n",
            "Epoch 7, loss : 0.17090469598770142, accuracy : 1.0\n",
            "Epoch 8, loss : 0.12679225206375122, accuracy : 0.9375\n",
            "Epoch 9, loss : 0.1403859555721283, accuracy : 0.875\n",
            "Test accuracy : 0.6626666784286499\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aQx28Ab_NdnU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "cf0f6c66-0046-478d-baf1-3ad55f7518f5"
      },
      "cell_type": "code",
      "source": [
        "labels = ['Negative', 'Positive']\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init_op)\n",
        "  prediction_ = sess.run(predictions, feed_dict={input_features: x_test[1].reshape(-1, x_test.shape[1])})\n",
        "  print(prediction_)\n",
        "  print('predicted class : {}, true class : {}'.format(sess.run(tf.argmax(prediction_, 1)), labels[sess.run(tf.argmax(y_test[1]))]))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00221592 0.9977841 ]]\n",
            "predicted class : [1], true class : Negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oDJfrmONUaMe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "baf9cd5e-f13e-4db0-c007-1c58ad5d53b9"
      },
      "cell_type": "code",
      "source": [
        "features[100]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i find it so amazing that even after all these years  we are still talking about this movie  obviously this movie wasn  t that bad or else people wouldn  t even bother to talk about it . i personally enjoyed this film immensly  and still do  i guess this film isn  t for everyone  but it certainly did touch the hearts of many .  br    br   as for those that think that this film is  overrated  or  over  hyped  . . . well  we only have the movie  going public to thank for that  lol you see  it  s not critics  article writers that make a film  huge  or a  hit  with the general movie  going public . people make the film a huge success . with titanic  everyone was in awe . let  s face it  a film like this had never been made before . at least not with the type of special effects needed to really capture the essence of the ship actually sinking . this film is so accurate that even james cameron timed the actual sinking of the ship in the film with the real sinking that fateful day in april     . even the silverware for goodness sakes matched   br    br   give this movie a break you guys  the critics thought this movie would sink big time  when this movie actually came out and people started hearing by word of mouth  which is the best form of advertisement mind you  that this was a good  decent  movie worth seeing  then everyone started flocking to the theaters in droves to see this movie . . . not once  not twice  but maybe  times and more  so  i really wouldn  t say that this movie was  overhyped  . . . at least not like the buildup for the matrix reloaded or the hulk is being  overhyped  . ha  critics didn  t even think that titanic would make enough money to cover cameron  s gigantic film budget that it took to make this mammoth of a film . however  the films money took care of that    million budget and much more   br    br   personally  i love this film . however  this film might not be for everyone . don  t say that this film sucks just because of romance though  that is the most sexist thing i  ve ever heard  disliking a movie just because it has romance in it  the story was sweet . the dialogue could have been better  but let  s face it . . . the real star of the movie wasn  t leo or kate . . . it was that gigantic ship  i think all of the actors including dicaprio and winslet did a fine job . it  s not thier best work  i  ve seen much better work from both of them  but it wasn  t the worst i  ve seen on screen before . give them a break   br    br    '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "0bQ7wJOfUg2B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "927a82c2-d390-4f74-e71e-4697ec1470c1"
      },
      "cell_type": "code",
      "source": [
        "labels[100]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-992ed79f5df8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m99\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "_hO1An_PGvC7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W-0XXsaKW5GQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ngrams_example = vec.transform(['the food was extremely bad!']).toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "buuh0t8DXW7t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "f738786a-432e-4b56-b16e-d9f787f9df20"
      },
      "cell_type": "code",
      "source": [
        "labels = ['Negative', 'Positive']\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init_op)\n",
        "  prediction_ = sess.run(predictions, feed_dict={input_features: ngrams_example.reshape(-1, ngrams_example.shape[1])})\n",
        "  print(prediction_)\n",
        "  print('predicted class : {}'.format(sess.run(tf.argmax(prediction_, 1))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.67699635 0.32300368]]\n",
            "predicted class : [0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}